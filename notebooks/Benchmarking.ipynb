{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and clean benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fn(df, col_name='text'):\n",
    "    \"\"\"Clean function used for preparing train/val data\"\"\"\n",
    "    \n",
    "    df.loc[:, col_name] = df[col_name].str.replace('https?://\\S+|www\\.\\S+', ' social medium ')\n",
    "    df.loc[:, col_name] = df[col_name].str.replace('\\s+', ' ')  # remove more than 1 white space\n",
    "    df.loc[:, col_name] = df[col_name].str.strip()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dir = Path('')\n",
    "\n",
    "benchmark_data = pd.read_csv(benchmark_dir/'validation_data.csv')\n",
    "benchmark_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data = clean_fn(benchmark_data.copy(), col_name='less_toxic')\n",
    "benchmark_data = clean_fn(benchmark_data.copy(), col_name='more_toxic')\n",
    "\n",
    "benchmark_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic_encoded, more_toxic_encoded = [], []\n",
    "\n",
    "for idx in tqdm(benchmark_data.index.values, total=benchmark_data.shape[0]):\n",
    "    lt_encoded = tokenizer.encode_plus(\n",
    "        benchmark_data.loc[idx, 'less_toxic'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    mt_encoded = tokenizer.encode_plus(\n",
    "        benchmark_data.loc[idx, 'more_toxic'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    less_toxic_encoded.append(lt_encoded)\n",
    "    more_toxic_encoded.append(mt_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path data threw model and get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic_scores, more_toxic_scores = [], []\n",
    "\n",
    "batch_size = 512\n",
    "steps = len(less_toxic_encoded) // batch_size + 1\n",
    "\n",
    "for idx in tqdm(range(steps)):\n",
    "    \n",
    "    minibatch = less_toxic_encoded[idx * batch_size:(idx + 1) * batch_size]\n",
    "    \n",
    "    lt_input_ids = [sample['input_ids'] for sample in minibatch]\n",
    "    lt_attn_mask = [sample['attention_mask'] for sample in minibatch]\n",
    "    \n",
    "    lt_model_inp = np.stack([lt_input_ids, lt_attn_mask], axis=-1)\n",
    "    \n",
    "    lt_scores = model(lt_model_inp, training=False).numpy().squeeze()\n",
    "    \n",
    "    less_toxic_scores.extend(lt_scores)\n",
    "\n",
    "for idx in tqdm(range(steps)):\n",
    "    \n",
    "    minibatch = more_toxic_encoded[idx * batch_size:(idx + 1) * batch_size]\n",
    "    \n",
    "    mt_input_ids = [sample['input_ids'] for sample in minibatch]\n",
    "    mt_attn_mask = [sample['attention_mask'] for sample in minibatch]\n",
    "    \n",
    "    mt_model_inp = np.stack([mt_input_ids, mt_attn_mask], axis=-1)\n",
    "    \n",
    "    mt_scores = model(mt_model_inp, training=False).numpy().squeeze()\n",
    "    \n",
    "    more_toxic_scores.extend(mt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores  = []\n",
    "for score_less_toxic_comment, score_more_toxic_comment in zip(less_toxic_scores, more_toxic_scores):\n",
    "    if score_less_toxic_comment < score_more_toxic_comment:\n",
    "        scores.append(1)\n",
    "    else:\n",
    "        scores.append(0)\n",
    "\n",
    "total_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {total_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
